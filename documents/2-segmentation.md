# Segmentation

## Purpose

In order to generate single-cell image patches, we must have some segmentation tool.  This document outlines how to generate segmentations from .npy files built in `1-preprocessing.py`, and how to perform instance segmentation on those.


The relevant CLI is:
```text
python run_segmentation.py -m <method> -c <path to config file>
```

where <method> is one of "segmentation" or "instance_segmentation" and
where <path to config file> is the full path to a .yml configuration file as specified in `.configs/config_example.yml`

--------------------------------------------
#### **method = "segmentation"**

This method selection will generate a "NNProbabiliites.npy" file for each of the sites specified in the "FOV" field, given a model architecture (only 'UNet' is supported currently) and a path to the UNet weights (defined in the config file). 

```text
python run_segmentation.py -m segmentation -c myconfig.yml
```

where `myconfig.yml` contains fields under `segmentation_inference`:
```text
segmentation_inference:
    model: 'UNet'
    weights: <full path to weights filename as .h5 file>
    fov: ['C4-Site_5', 'C4-Site_1', etc...]
```

**inputs**
From "raw" directory
- reads `<fov>.npy` file generated from `run_preproc.py`

From any directory
- reads `<my weights file>.h5` generated by whatever UNet training procedure you choose

**outputs**
To "raw" directory
- writes `<fov>_NNProbabilities.npy` --> will be of shape (T, Y, X, C), where C is 3 in the case of the dynamorph paper
- writes `<fov>_NNpred.png`

-------------------------------------------
#### **method = "instance_segmentation"**

This method selection will use the raw data and the probability map to generate labels and mappings to cell-instances from the segmentation

```text
python run_segmentation.py -m instance_segmentation -c myconfig.yml
```

instance segmentation is done using the clustering method DBSCAN (sklearn.cluster).  The process is as follows:

```text
for each time point
1. filter cells whose probability qualifies it for "foreground".  This is "fg_thr" < 0.3 in the paper.
2. perform DBSCAN clustering with `eps = 10` and `min_samples = 250` (values used in dynamorph paper)
3. position_labels is the output of step 2
4. cell_ids, point_counts is set of unique values from position_labels
5. for each cell_id/point_counts
        define a "mean position" around each cluster
        define a window of 256x256 around that mean
        exclude clusters that have too many outliers outside that window (> 5% of points) 
6.      append (cell_id, mean_pos) to qualifying cells to the `cell_positions` list
7. assign the output of 6 to the dictionary `cell_positions[time_point]`
```

**inputs**
From "raw" directory
- reads `<fov>.npy`
- reads `<fov>_NNProbabilities.npy`

**outputs**
To "<fov>-supps/<fov>" directory
- writes `cell_positions.pkl`
- writes `cell_pixel_assignments.pkl`

where `cell_positions.pkl` is a dictionary of {key:value} = {timepoint: (microglia-cell-map, non-microglia-cell-map, other-cell-map)}  
and where `<MG or nonMG or other>-cell-map` represents `[ (cell_id, np.array(mean-x-pos, mean-y-pos)), (next_cell_id, np.array(mean-x-pos, mean-y-pos)), ... ]`


where `cell_pixel_assignments.pkl` is a dictionary of {key:value} = {timepoint: (positions, position_labels)}
and where `positions` represents array of (X, Y) coordinates of foreground pixels  
and where `position_labels` represents an array of cell_IDs of those foreground pixels  
